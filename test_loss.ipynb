{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Permute, Reshape, Conv2D, BatchNormalization, Activation,Add,AveragePooling2D,Flatten,Dense\n",
    "import time\n",
    "from tensorflow.keras.regularizers import l2,l1_l2\n",
    "from tensorflow.keras.losses import Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self,ensemble_size,batch_rep, inp_rep_prob,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.batch_rep = batch_rep\n",
    "        self.inp_rep_prob = inp_rep_prob\n",
    "        self.ensemble_size = ensemble_size\n",
    "\n",
    "    def train_step(self, data):\n",
    "        imgs, labels = data\n",
    "        batch_size = tf.shape(imgs)[0]\n",
    "        batch_rep = tf.tile(tf.range(batch_size),[self.batch_rep])\n",
    "        shuffled_batch_rep = tf.shuffle(batch_rep)\n",
    "        input_shuffle=tf.cast(tf.cast(batch_size,tf.float32) * (1. - self.inp_rep_prob),tf.int32)\n",
    "        #Kan detta göras bättre?\n",
    "        shuffle_idxs = [tf.concat([tf.random.shuffle(shuffled_batch_rep[:input_shuffle]), input_shuffle[input_shuffle:]], axis=0) for _ in range(self.ensemble_size)]\n",
    "        imgs = tf.stack([tf.gather(imgs, indxs, axis=0) for indxs in shuffle_idxs], axis=1)\n",
    "        labels = tf.stack([tf.gather(labels, indxs, axis=0) for indxs in shuffle_idxs], axis=1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(imgs, training=True)\n",
    "            loss = self.compiled_loss(\n",
    "                labels,\n",
    "                imgs,\n",
    "                regularization_losses=self.losses,\n",
    "            )\n",
    "\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return #TODO Metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        imgs, labels = data\n",
    "        imgs = tf.tile(tf.expand_dims(imgs, 1), [1, self.ensemble_size, 1, 1, 1]) #Expand to add ensemble dimension\n",
    "        logits = self(imgs, training=False)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "        probs = tf.math.reduce_mean(probs, axis=1) \n",
    "\n",
    "\n",
    "        return #TODO Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_block(input,filters,strides,l_2):\n",
    "    y = input\n",
    "    x = BatchNormalization(momentum=0.9,epsilon=1e-5,beta_regularizer=l2(l_2),gamma_regularizer=l2(l_2))(input)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters,3,strides=strides,padding ='same',use_bias=False,kernel_initializer=\"he_normal\",kernel_regularizer=l2(l_2))(x)\n",
    "    x = BatchNormalization(momentum=0.9,epsilon=1e-5,beta_regularizer=l2(l_2),gamma_regularizer=l2(l_2))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters,3,strides=1,padding ='same',use_bias=False,kernel_initializer=\"he_normal\",kernel_regularizer=l2(l_2))(x)\n",
    "    \n",
    "    if not x.shape.is_compatible_with(y.shape):\n",
    "        y = Conv2D(filters,1,strides=strides,padding ='same',use_bias=False,kernel_initializer=\"he_normal\",kernel_regularizer=l2(l_2))(input)\n",
    "\n",
    "    return Add()([x,y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_group(input,filters,strides,n_blocks,l_2):\n",
    "    x = basic_block(input,filters,strides,l_1,l_2)\n",
    "    for _ in range(n_blocks-1):\n",
    "        x = basic_block(x,filters,1,l_1,l_2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_resnet(input_shape,d,w_mult,n_classes,batch_rep,inp_rep_prob, l_2=0):\n",
    "    n_blocks = (d - 4) // 6\n",
    "    input_shape = list(input_shape)\n",
    "    ensemble_size = input_shape[0]\n",
    "\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Permute([2,3,4,1])(input)\n",
    "\n",
    "    # Reshape so that each subnetwork has 3 channels\n",
    "    x = Reshape(input_shape[1:-1] + [input_shape[-1] * ensemble_size])(x)\n",
    "\n",
    "\n",
    "    x = Conv2D(16,3,padding ='same',use_bias=False,kernel_initializer=\"he_normal\",kernel_regularizer=l2(l_2))(x)\n",
    "\n",
    "    for strides, filters in zip([1, 2, 2], [16, 32, 64]):\n",
    "        x = res_group(x,filters*w_mult,strides,n_blocks,l_2)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9,epsilon=1e-5,beta_regularizer=l2(l_2),gamma_regularizer=l2(l_2))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    x = Dense(n_classes*ensemble_size,kernel_initializer='he_normal',activation=None,kernel_regularizer=l2(l_2),bias_regularizer=l2(l_2))(x)\n",
    "    x = Reshape([batch_size,ensemble_size,n_classes])(x)\n",
    "    \n",
    "    return tf.keras.CusomModel(ensemble_size, batch_rep,inp_rep_prob ,input=input,output=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Loss):\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.convert_to_tensor_v2(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        nll = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True), axis=1))\n",
    "        return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wide_resnet(input_shape,d,w_mult,n_classes,batch_rep,inp_rep_prob, l_2=0,l_1=0)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.1,\n",
    "    decay_steps=steps_per_epoch,\n",
    "    decay_rate=0.1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr_schedule, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer,loss = NLL())\n",
    "model.fit(x_train,y_train,batch_size,validation_split=val_split)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c7a171be4500626eafa29c013678d2d3ba3bac8c79e4ce1e00bc0ee886ad94e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
