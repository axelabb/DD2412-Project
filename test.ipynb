{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Permute, Reshape, Conv2D, BatchNormalization, Activation,Add,AveragePooling2D,Flatten,Dense\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Parser')\n",
    "parser.add_argument('epochs',type=int,default=2)\n",
    "#TODO\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_block(input,filters,strides):\n",
    "    y = input\n",
    "    x = BatchNormalization(momentum=0.9,epsilon=1e-5)(input)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters,3,strides=strides,padding ='same',use_bias=False,kernel_initializer=\"he_normal\")(x)\n",
    "    x = BatchNormalization(momentum=0.9,epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters,3,strides=1,padding ='same',use_bias=False,kernel_initializer=\"he_normal\")(x)\n",
    "    \n",
    "    if not x.shape.is_compatible_with(y.shape):\n",
    "        y = Conv2D(filters,1,strides=strides,padding ='same',use_bias=False,kernel_initializer=\"he_normal\")(input)\n",
    "\n",
    "    return Add()([x,y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_group(input,filters,strides,n_blocks):\n",
    "    x = basic_block(input,filters,strides)\n",
    "    for _ in range(n_blocks):\n",
    "        x = basic_block(x,filters,1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_resnet(input_shape,d,w_mult,n_classes):\n",
    "    n_blocks = (d - 4) // 6\n",
    "    input_shape = list(input_shape)\n",
    "    ensemble_size = input_shape[0]\n",
    "\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Permute([2,3,4,1])(input)\n",
    "\n",
    "    # Reshape so that each subnetwork has 3 channels\n",
    "    x = Reshape(input_shape[1:-1] + [input_shape[-1] * ensemble_size])\n",
    "\n",
    "    x = Conv2D(16,3,padding ='same',use_bias=False,kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    for strides, filters in zip([1, 2, 2], [16, 32, 64]):\n",
    "        x = res_group(x,filters*w_mult,strides,n_blocks)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9,epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    #TODO Dense multi head\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    x = Dense(n_classes*ensemble_size,kernel_initializer='he_normal',activation=None)(x)\n",
    "    x = Reshape([batch_size,ensemble_size,n_classes])(x)\n",
    "    \n",
    "    return tf.keras.Model(input=input,output=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_10():\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model,data,args):\n",
    "    #TODO\n",
    "    imgs, labels = data\n",
    "    batch_size = tf.shape(imgs)[0]\n",
    "    batch_rep = tf.tile(tf.range(batch_size),[args.batch_rep])\n",
    "    shuffled_batch_rep = tf.shuffle(batch_rep)\n",
    "    input_shuffle=tf.cast(tf.cast(batch_size,tf.float32) * (1. - args.inp_rep_prob),tf.int32)\n",
    "    #Kan detta göras bättre?\n",
    "    shuffle_idxs = [tf.concat([tf.random.shuffle(shuffled_batch_rep[:input_shuffle]), input_shuffle[input_shuffle:]], axis=0) for _ in range(args.ensemble_size)]\n",
    "    imgs = tf.stack([tf.gather(imgs, indxs, axis=0) for indxs in shuffle_idxs], axis=1)\n",
    "    labels = tf.stack([tf.gather(labels, indxs, axis=0) for indxs in shuffle_idxs], axis=1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(imgs, training=True)\n",
    "        nll = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True), axis=1))\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,dataset,args):\n",
    "    steps_per_epoch = dataset.size // args.batch_size\n",
    "    train_iter=iter(dataset)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"Starting epoch {epoch}\")\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        for step in range(steps_per_epoch):\n",
    "            train_step(model,next(train_iter))\n",
    "            \n",
    "        print(f\"Epoch {epoch} took {time.time()-start_time_epoch} total elapsed time {time.time()-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c7a171be4500626eafa29c013678d2d3ba3bac8c79e4ce1e00bc0ee886ad94e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
